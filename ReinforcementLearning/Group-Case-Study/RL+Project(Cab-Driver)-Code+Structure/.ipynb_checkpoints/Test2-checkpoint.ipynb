{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    c = CabDriver()\n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01       \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01 \n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "#     def build_model(self):\n",
    "#         model = Sequential()\n",
    "#         # Write your code here: Add layers to your neural nets      \n",
    "#         # hidden layers\n",
    "        \n",
    "#         model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "#         model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "#         # the output layer: output is of size num_actions\n",
    "#         model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "#         model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "#         model.summary\n",
    "#         return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state, possible_actions_index, actions):\n",
    "        # Write your code here:\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after we generate each sample from the environment \n",
    "        \n",
    "        epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay*time)\n",
    "        # print(epsilon)\n",
    "        z = np.random.random()\n",
    "        \n",
    "\n",
    "        if z <= self.epsilon:\n",
    "            # explore: choose a random action from the ride requests\n",
    "            return random.choice(possible_actions_index)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = c.state_encod_arch1(state)\n",
    "\n",
    "            # Use the model to predict the Q_values.\n",
    "            q_value = self.model.predict(state)\n",
    "\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "\n",
    "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
    "        \n",
    "    \n",
    "\n",
    "#     def append_sample(self, state, action, reward, next_state):\n",
    "#         # Write your code here:\n",
    "#         # save sample <s,a,r,s'> to the replay memory\n",
    "    \n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "#     def train_model(self):\n",
    "#         if len(self.memory) > self.batch_size:\n",
    "#             # Sample batch from the memory\n",
    "#             mini_batch = random.sample(self.memory, self.batch_size)\n",
    "#             update_output = # write here\n",
    "#             update_input = # write here\n",
    "#             action, reward = [], []\n",
    "            \n",
    "#             for i in range(self.batch_size):\n",
    "#                 state, action, reward, next_state = mini_batch[i]\n",
    "#                 state_encod = env.state_encod_arch2(state,action)\n",
    "                           \n",
    "                \n",
    "#                 # Write your code from here\n",
    "#                 # 1. Predict the target from earlier model\n",
    "                \n",
    "                \n",
    "#                 # 2. Get the target for the Q-network\n",
    "                \n",
    "                \n",
    "#                 #3. Update your 'update_output' and 'update_input' batch. Be careful to use the encoded state-action pair\n",
    "\n",
    "                \n",
    "                \n",
    "#         # 4. Fit your model and track the loss values\n",
    "\n",
    "\n",
    "\n",
    "#     def save(self, name):\n",
    "#         self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
